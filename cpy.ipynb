{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting ground state of molecule using Hybrid quantum-classical Neural Networks(FFNN) (please run all before look at it, I dont have time to train)\n",
    "## Project Description - poig\n",
    "I am a highschool student, unable find any free team so decide to solo it. This is my first AI project, without much knowledgth about AI.  \n",
    "I dont have credit card, cant use aws power up redeem code.\n",
    "\n",
    "my linkedin - linkedin.com/in/jun-liang-tan-0b8b53133/\n",
    "### goal:\n",
    "predict the ground state energies of molecules using Coulomb matrices as input using quantum-classical Neural Networks. I hope to combine advanced feed forward neural network(FFNN) and quantum neural network(QNN) to achieve higher accuracy.\n",
    "\n",
    "\n",
    "It will train over 16,242 Molecules ground state with FNN&QNN.\n",
    "[training databset from kaggle](https://www.kaggle.com/burakhmmtgl/energy-molecule/code)\n",
    "\n",
    "### reference (those I think is important)- \n",
    "[1] [Predicting excited states from ground state wavefunction by supervised quantum machine learning Hiroki Kawai, Yuya O. Nakagawa](https://www.arxiv.org/pdf/2002.12925.pdf) (where my idea come from)   \n",
    "[2] [Tree based machine learning framework for predicting ground state energies of molecules](https://www.arxiv.org/pdf/1609.07124.pdf)  (where the data set from, can download from kaggle)  \n",
    "[3] [qiskit- hybrid neural network](https://www.qiskit.org/textbook/ch-machine-learning/machine-learning-qiskit-pytorch.html) (where I get idea how to do it)   \n",
    "[4] [pennylane- recurrent neural network lstm](https://www.pennylane.ai/qml/demos/learning2learn.html#recurrent-neural-network-lstm) (where I get idea of####)  \n",
    "[5] [github- AdvancedNNfromScratch](https://www.github.com/mukul-rathi/ChemRegressionNeuralNet/blob/master/AdvancedNNfromScratch.ipynb) (where I understand step need for this project)  \n",
    "[6] [Can One Hear the Shape of a Molecule (from its Coulomb Matrix Eigenvalues)? Joshua Schrier Journal of Chemical Information and Modeling 2020 60 (8), 3804-3811 DOI: 10.1021/acs.jcim.0c00631](https://pubs.acs.org/doi/10.1021/acs.jcim.0c00631) (where I understand little about coulomb matrix, how much thing can do with it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project\n",
    "## Contents\n",
    "\n",
    "1. [setup data](#setup-data)\n",
    "2. [hyperparameter](#hyper-parameter-that-can-tune-to-improve-traning)\n",
    "3.  \n",
    "4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x225ff115270>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import qiskit\n",
    "from qiskit import QuantumCircuit, transpile, Aer ,  execute, BasicAer, assemble\n",
    "from qiskit.visualization import *\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as u\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "import networkx as nx\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "#import sklearn.preprocessing import StandardScaler\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup data<a id=\"data\"></a>\n",
    "### Data set - kaggle.com/burakhmmtgl/energy-molecule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    16242.000000\n",
       "mean       -11.178969\n",
       "std          3.659133\n",
       "min        -23.245373\n",
       "25%        -13.475805\n",
       "50%        -10.835211\n",
       "75%         -8.623903\n",
       "max         -0.789513\n",
       "Name: Eat, dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('roboBohr.csv') #create a new dataframe containing the input data\n",
    "dataset.Eat.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>1267</th>\n",
       "      <th>1268</th>\n",
       "      <th>1269</th>\n",
       "      <th>1270</th>\n",
       "      <th>1271</th>\n",
       "      <th>1272</th>\n",
       "      <th>1273</th>\n",
       "      <th>1274</th>\n",
       "      <th>pubchem_id</th>\n",
       "      <th>Eat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>73.516695</td>\n",
       "      <td>17.817765</td>\n",
       "      <td>12.469551</td>\n",
       "      <td>12.458130</td>\n",
       "      <td>12.454607</td>\n",
       "      <td>12.447345</td>\n",
       "      <td>12.433065</td>\n",
       "      <td>12.426926</td>\n",
       "      <td>12.387474</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25004</td>\n",
       "      <td>-19.013763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>73.516695</td>\n",
       "      <td>20.649126</td>\n",
       "      <td>18.527789</td>\n",
       "      <td>17.891535</td>\n",
       "      <td>17.887995</td>\n",
       "      <td>17.871731</td>\n",
       "      <td>17.852586</td>\n",
       "      <td>17.729842</td>\n",
       "      <td>15.864270</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25005</td>\n",
       "      <td>-10.161019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>73.516695</td>\n",
       "      <td>17.830377</td>\n",
       "      <td>12.512263</td>\n",
       "      <td>12.404775</td>\n",
       "      <td>12.394493</td>\n",
       "      <td>12.391564</td>\n",
       "      <td>12.324461</td>\n",
       "      <td>12.238106</td>\n",
       "      <td>10.423249</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25006</td>\n",
       "      <td>-9.376619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>73.516695</td>\n",
       "      <td>17.875810</td>\n",
       "      <td>17.871259</td>\n",
       "      <td>17.862402</td>\n",
       "      <td>17.850920</td>\n",
       "      <td>17.850440</td>\n",
       "      <td>12.558105</td>\n",
       "      <td>12.557645</td>\n",
       "      <td>12.517583</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25009</td>\n",
       "      <td>-13.776438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>73.516695</td>\n",
       "      <td>17.883818</td>\n",
       "      <td>17.868256</td>\n",
       "      <td>17.864221</td>\n",
       "      <td>17.818540</td>\n",
       "      <td>12.508657</td>\n",
       "      <td>12.490519</td>\n",
       "      <td>12.450098</td>\n",
       "      <td>10.597068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25011</td>\n",
       "      <td>-8.537140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1278 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0          0          1          2          3          4  \\\n",
       "0           0  73.516695  17.817765  12.469551  12.458130  12.454607   \n",
       "1           1  73.516695  20.649126  18.527789  17.891535  17.887995   \n",
       "2           2  73.516695  17.830377  12.512263  12.404775  12.394493   \n",
       "3           3  73.516695  17.875810  17.871259  17.862402  17.850920   \n",
       "4           4  73.516695  17.883818  17.868256  17.864221  17.818540   \n",
       "\n",
       "           5          6          7          8  ...  1267  1268  1269  1270  \\\n",
       "0  12.447345  12.433065  12.426926  12.387474  ...   0.0   0.0   0.5   0.0   \n",
       "1  17.871731  17.852586  17.729842  15.864270  ...   0.0   0.0   0.0   0.0   \n",
       "2  12.391564  12.324461  12.238106  10.423249  ...   0.0   0.0   0.0   0.0   \n",
       "3  17.850440  12.558105  12.557645  12.517583  ...   0.0   0.0   0.0   0.0   \n",
       "4  12.508657  12.490519  12.450098  10.597068  ...   0.0   0.0   0.0   0.0   \n",
       "\n",
       "   1271  1272  1273  1274  pubchem_id        Eat  \n",
       "0   0.0   0.0   0.0   0.0       25004 -19.013763  \n",
       "1   0.0   0.0   0.0   0.0       25005 -10.161019  \n",
       "2   0.0   0.0   0.0   0.0       25006  -9.376619  \n",
       "3   0.0   0.0   0.0   0.0       25009 -13.776438  \n",
       "4   0.0   0.0   0.0   0.0       25011  -8.537140  \n",
       "\n",
       "[5 rows x 1278 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split- train set, validation, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (16242, 1275)\n",
      "Y shape:  (16242, 1)\n",
      "X Y train,test shape:  (13886, 1275) (1625, 1275) (13886, 1) (1625, 1)\n",
      "X Y train,validation shape:  (13886, 1275) (731, 1275) (13886, 1) (731, 1)\n"
     ]
    }
   ],
   "source": [
    "df = dataset.drop(['Unnamed: 0','pubchem_id'],axis=1)\n",
    "X = df.drop(['Eat'], axis = 1).values\n",
    "y = df['Eat'].values.reshape((-1,1))\n",
    "print(\"X shape: \",X.shape)\n",
    "print(\"Y shape: \",y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,  test_size=0.10, random_state=42)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train,  test_size=0.05, random_state=42)\n",
    "print(\"X Y train,test shape: \",X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "print(\"X Y train,validation shape: \",X_train.shape, X_validation.shape, y_train.shape, y_validation.shape)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "y_train = scaler.fit_transform(y_train)\n",
    "y_test = scaler.transform(y_test)\n",
    "y_validation = scaler.transform(y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13886, 471)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_select = VarianceThreshold(threshold=0.05)\n",
    "X_train = feature_select.fit_transform(X_train)\n",
    "X_validation = feature_select.transform(X_validation)\n",
    "X_test = feature_select.transform(X_test)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[388.0234,  28.0980,  27.9802,  ...,   0.0000,   0.0000,   0.0000],\n",
       "        [ 53.3587,  19.1476,  19.1469,  ...,   0.0000,   0.0000,   0.0000],\n",
       "        [ 73.5167,  20.6570,  19.1436,  ...,   0.0000,   0.0000,   0.0000],\n",
       "        ...,\n",
       "        [ 53.3587,  16.1596,  16.1586,  ...,   0.0000,   0.0000,   0.0000],\n",
       "        [ 53.3587,  19.1477,  17.1931,  ...,   0.5000,   0.5000,   0.5000],\n",
       "        [ 73.5167,  20.8886,  20.7831,  ...,   0.0000,   0.0000,   0.0000]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# If you don't have a GPU, buy a graphics card. I have for a long time used a 1060 GTX, which is not that expensive anymore.\n",
    "X_train = torch.tensor(X_train, device=device).float()\n",
    "X_test = torch.tensor(X_test, device=device).float()\n",
    "X_validation = torch.tensor(X_validation, device=device).float()\n",
    "y_train = torch.tensor(y_train, device=device).float()\n",
    "y_test = torch.tensor(y_test, device=device).float()\n",
    "y_validation = torch.tensor(y_validation, device=device).float()\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "validation_dataset = TensorDataset(X_validation, y_validation)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                              batch_size=256,\n",
    "                                              shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset,\n",
    "                                                  batch_size=256,\n",
    "                                                  shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create neural network\n",
    "### hyper-parameter that can tune to improve traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#can be easy tune when train\n",
    "hyperparameters={}\n",
    "hyperparameters[\"num_epochs\"] = 20 #number of passes through the training set\n",
    "hyperparameters[\"batch_size\"] = 128 #number of examples trained upon in each step of training\n",
    "#hyperparameters[\"layers_units\"] = [X_train.T.shape[0], 128, 64, 32, 1] #layer 0 is the input layer\n",
    "hyperparameters[\"hidden_size\"] = 100\n",
    "hyperparameters[\"input_size\"] = X_train.T.shape[0]\n",
    "hyperparameters[\"learning_rate\"] = 1e-3 #learning rate\n",
    "hyperparameters[\"dropout\"] =0.8 #probability blacked out\n",
    "hyperparameters[\"training_size\"] = 0.01  #16,242 max\n",
    "\n",
    "#hyperparameters[\"layers_units\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([99, 471])\n",
      "471\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:99,:].shape)\n",
    "print(X_train.T.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load account and dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\junli\\anaconda3\\lib\\site-packages\\qiskit\\providers\\ibmq\\ibmqfactory.py:192: UserWarning: Timestamps in IBMQ backend properties, jobs, and job results are all now in local time instead of UTC.\n",
      "  warnings.warn('Timestamps in IBMQ backend properties, jobs, and job results '\n",
      "ibmqfactory.load_account:WARNING:2022-02-27 02:33:30,227: Credentials are already in use. The existing account in the session will be replaced.\n"
     ]
    }
   ],
   "source": [
    "from qiskit import QuantumCircuit, transpile, Aer, IBMQ\n",
    "IBMQ.save_account('fb17afa861c2170c581e883349da83b1d508fa7a89d49714e76852f51f874de76988e70ad33b4a49b32d4fffb8f58c7c3abd2ab5be5d4b909bcc1af28e280848',overwrite=True)  #save your creds\n",
    "qiskit.IBMQ.load_account()\n",
    "provider = IBMQ.load_account()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here is the list of cloud backends that are available to you:\n",
      "ibmq_qasm_simulator\n",
      "ibmq_armonk\n",
      "ibmq_santiago\n",
      "ibmq_bogota\n",
      "ibmq_lima\n",
      "ibmq_belem\n",
      "ibmq_quito\n",
      "simulator_statevector\n",
      "simulator_mps\n",
      "simulator_extended_stabilizer\n",
      "simulator_stabilizer\n",
      "ibmq_manila\n"
     ]
    }
   ],
   "source": [
    "available_cloud_backends = provider.backends() \n",
    "print('\\nHere is the list of cloud backends that are available to you:')\n",
    "for i in available_cloud_backends: print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantum circuit with qiskit (can apply complicated circuit, to archive quantum advantage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumCircuit:\n",
    "    def __init__(self, n_qubits, backend, shots):\n",
    "        # --- Circuit definition ---\n",
    "        self._circuit = qiskit.QuantumCircuit(n_qubits)\n",
    "        \n",
    "        all_qubits = [i for i in range(n_qubits)]\n",
    "        self.theta = qiskit.circuit.Parameter('theta')\n",
    "        \n",
    "        self._circuit.h(all_qubits)\n",
    "        self._circuit.barrier()\n",
    "        self._circuit.ry(self.theta, all_qubits)\n",
    "        \n",
    "        self._circuit.measure_all()\n",
    "        # ---------------------------\n",
    "\n",
    "        self.backend = backend\n",
    "        self.shots = shots\n",
    "    \n",
    "    def run(self, thetas):\n",
    "        t_qc = transpile(self._circuit,\n",
    "                         self.backend)\n",
    "        qobj = assemble(t_qc,\n",
    "                        shots=self.shots,\n",
    "                        parameter_binds = [{self.theta: theta} for theta in thetas])\n",
    "        job = self.backend.run(qobj)\n",
    "        result = job.result().get_counts()\n",
    "        \n",
    "        counts = np.array(list(result.values()))\n",
    "        states = np.array(list(result.keys())).astype(float)\n",
    "        \n",
    "        # Compute probabilities for each state\n",
    "        probabilities = counts / self.shots\n",
    "        # Get state expectation\n",
    "        expectation = np.sum(states * probabilities)\n",
    "        \n",
    "        return np.array([expectation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected value for rotation pi 0.47\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"word-wrap: normal;white-space: pre;background: #fff0;line-height: 1.1;font-family: &quot;Courier New&quot;,Courier,monospace\">        ┌───┐ ░ ┌───────────┐ ░ ┌─┐\n",
       "   q_0: ┤ H ├─░─┤ RY(theta) ├─░─┤M├\n",
       "        └───┘ ░ └───────────┘ ░ └╥┘\n",
       "meas: 1/═════════════════════════╩═\n",
       "                                 0 </pre>"
      ],
      "text/plain": [
       "        ┌───┐ ░ ┌───────────┐ ░ ┌─┐\n",
       "   q_0: ┤ H ├─░─┤ RY(theta) ├─░─┤M├\n",
       "        └───┘ ░ └───────────┘ ░ └╥┘\n",
       "meas: 1/═════════════════════════╩═\n",
       "                                 0 "
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test output\n",
    "backend = qiskit.Aer.get_backend('qasm_simulator')\n",
    "#backend = provider.get_backend('ibmq_qasm_simulator')\n",
    "\n",
    "circuit = QuantumCircuit(1, backend, 100)\n",
    "print('Expected value for rotation pi {}'.format(circuit.run([np.pi])[0]))\n",
    "circuit._circuit.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantum-Classical with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "class HybridFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, quantum_circuit, shift):\n",
    "        \"\"\" Forward pass computation \"\"\"\n",
    "        ctx.shift = shift\n",
    "        ctx.quantum_circuit = quantum_circuit\n",
    "\n",
    "        expectation_z = ctx.quantum_circuit.run(input[0].tolist())\n",
    "        result = torch.tensor([expectation_z])\n",
    "        ctx.save_for_backward(input, result)\n",
    "\n",
    "        return result\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\" Backward pass computation \"\"\"\n",
    "        input, expectation_z = ctx.saved_tensors\n",
    "        input_list = np.array(input.tolist())\n",
    "        \n",
    "        \n",
    "        shift_right = input_list + np.ones(input_list.shape) * ctx.shift\n",
    "        shift_left = input_list - np.ones(input_list.shape) * ctx.shift\n",
    "        \n",
    "        gradients = []\n",
    "        for i in range(len(input_list)):\n",
    "            expectation_right = ctx.quantum_circuit.run(shift_right[i])\n",
    "            expectation_left  = ctx.quantum_circuit.run(shift_left[i])\n",
    "            \n",
    "            gradient = torch.tensor([expectation_right]) - torch.tensor([expectation_left])\n",
    "            gradients.append(gradient)\n",
    "        gradients = np.array([gradients]).T\n",
    "        return torch.tensor([gradients]).float() * grad_output.float(), None, None\n",
    "\n",
    "class Hybrid(nn.Module):\n",
    "    def __init__(self,qubit, backend, shots, shift):\n",
    "        super(Hybrid, self).__init__()\n",
    "        self.quantum_circuit = QuantumCircuit(qubit, backend, shots)\n",
    "        self.shift = shift\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return HybridFunction.apply(input, self.quantum_circuit, self.shift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Hybrid Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HNN(nn.Module): \n",
    "    def __init__(self,inpt,hd,drop_out=hyperparameters[\"dropout\"]): #'''layer_units=hyperparameters[\"layers_units\"]'''\n",
    "        super(HNN, self).__init__()\n",
    "        #[setdata.drop(['Eat'], axis = 1).values.T.shape[0], 128, 64, 32, 1]\n",
    "        #layer_1,layer_2,layer_3,layer_4,layer_5 = layer_units\n",
    "        self.linear1 = nn.Linear(inpt, hd)\n",
    "        self.linear2 = nn.Linear(hd, hd)\n",
    "        self.linear3 = nn.Linear(hd, hd) \n",
    "        self.linear4 = nn.Linear(hd, 1) \n",
    "        self.hybrid = Hybrid(1,qiskit.Aer.get_backend('qasm_simulator'), 100, np.pi / 2)\n",
    "        #normalization layer\n",
    "        self.ln1 = nn.LayerNorm(hd)\n",
    "        self.ln2 = nn.LayerNorm(hd)\n",
    "        self.ln3 = nn.LayerNorm(hd)\n",
    "        #self.hybrid2 = Hybrid(1,provider.get_backend('ibmq_qasm_simulator'), 100, np.pi / 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.ln1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.dropout(out)#drop_out\n",
    "        \n",
    "        out = self.linear1(x)\n",
    "        out = self.ln1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.dropout(out)#drop_out\n",
    "        \n",
    "        out = self.linear3(out)\n",
    "        out = self.ln3(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        #out = self.dropout(out)\n",
    "        out = self.linear4(out)\n",
    "        #Final output layer\n",
    "        out = self.hybrid(out)\n",
    "        #out = self.hybrid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model(X_train,X_validation,y_train,y_validation, hyperparameters):\n",
    "    num_epochs = hyperparameters[\"num_epochs\"]#\n",
    "    batch_size = hyperparameters[\"batch_size\"]#\n",
    "    #layers_units = hyperparameters[\"layers_units\"] #bugs\n",
    "    inpt = hyperparameters[\"input_size\"]\n",
    "    hd = hyperparameters[\"hidden_size\"]\n",
    "    learning_rate = hyperparameters[\"learning_rate\"] #\n",
    "    drop_out = hyperparameters[\"dropout\"] # for dropout\n",
    "    tra_size = hyperparameters[\"training_size\"]\n",
    "    #tra_size = 1\n",
    "    trat_size =  int(np.round(tra_size* X_train.T.shape[0]))\n",
    "    trav_size =  int(np.round(tra_size* X_validation.T.shape[0]))\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    \n",
    "    X_train = torch.tensor(X_train[:trat_size,:], device=device).float()\n",
    "    X_validation = torch.tensor(X_validation[:trav_size,:], device=device).float()\n",
    "    \n",
    "    y_train = torch.tensor(y_train[:trat_size,:], device=device).float()\n",
    "    y_validation = torch.tensor(y_validation[:trav_size,:], device=device).float()\n",
    "    \n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    validation_dataset = TensorDataset(X_validation, y_validation)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True)\n",
    "    validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset,\n",
    "                                              batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    model = HNN(inpt, hd, drop_out)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    model.train() #Ensure the network is in \"train\" mode with dropouts active\n",
    "    epochs = num_epochs\n",
    "    for e in range(epochs):\n",
    "        loss = 0\n",
    "        for x, y in train_loader:\n",
    "            # Training pass\n",
    "            optimizer.zero_grad() # Initialize the gradients, which will be recorded during the forward pa\n",
    "             \n",
    "            output = model(x) #Forward pass of the mini-batch\n",
    "            loss = criterion(output, torch.tensor(y,dtype=torch.float32,requires_grad=True)) #Computing the loss\n",
    "            loss.backward() # calculate the backward pass\n",
    "            optimizer.step() # Optimize the weights\n",
    "             \n",
    "            loss += loss.item()\n",
    "        else:\n",
    "            if e%10 == 0:\n",
    "                validation_loss = torch.mean(( y_validation - model(X_validation) )**2).item()\n",
    "                print(\"Epoch: %3i Training loss: %0.2F Validation loss: %0.2F\"%(e,(loss/len(train_loader)), validation_loss))\n",
    "    \n",
    "    model.eval() #test\n",
    "    y_pred_train = model(X_train)\n",
    "    y_pred_validation = model(X_validation)\n",
    "    y_pred_test = model(X_test)\n",
    "    print(\"root_sqr_error\")\n",
    "    print(\"train error \",torch.mean(( y_train - y_pred_train )**2).item())\n",
    "    print(torch.mean(( y_validation - y_pred_validation )**2).item())\n",
    "    print(torch.mean(( y_test - y_pred_test )**2).item())\n",
    "    #print(model)\n",
    "    '''def flatten(tensor):\n",
    "        return tensor.cpu().detach().numpy().flatten()\n",
    "    plt.scatter(flatten(y_pred_test), flatten(y_test), alpha=0.5, label=\"Test\")\n",
    "    plt.scatter(flatten(y_pred_train), flatten(y_train), alpha=0.1, label=\"Train\")\n",
    "    plt.legend()\n",
    "    #plt.plot([-3,3], [-3,3], c=\"b\")\n",
    "    plt.plot([-1.5, 1.5], [-1.5,1.5], c=\"b\")'''\n",
    "    \n",
    "    \n",
    "    return model,device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-190-730e4c1a3a89>:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train[:trat_size,:], device=device).float()\n",
      "<ipython-input-190-730e4c1a3a89>:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_validation = torch.tensor(X_validation[:trav_size,:], device=device).float()\n",
      "<ipython-input-190-730e4c1a3a89>:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train = torch.tensor(y_train[:trat_size,:], device=device).float()\n",
      "<ipython-input-190-730e4c1a3a89>:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_validation = torch.tensor(y_validation[:trav_size,:], device=device).float()\n",
      "<ipython-input-190-730e4c1a3a89>:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = criterion(output, torch.tensor(y,dtype=torch.float32,requires_grad=True)) #Computing the loss\n",
      "C:\\Users\\junli\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:431: UserWarning: Using a target size (torch.Size([128, 1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\junli\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:431: UserWarning: Using a target size (torch.Size([121, 1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   0 Training loss: 0.59 Validation loss: 1.24\n",
      "root_sqr_error\n",
      "train error  1.0371835694648879\n",
      "1.5488676537043062\n",
      "1.2396226385359856\n"
     ]
    }
   ],
   "source": [
    "#prev_parameters = copy.deepcopy(hyperparameters)\n",
    "hyperparameters[\"training_size\"] = 0.8\n",
    "hyperparameters[\"num_epochs\"] = 10\n",
    "hyperparameters[\"learning_rate\"] = 1e-2\n",
    "hyperparameters[\"hidden_size\"] = 100\n",
    "\n",
    "model,device = training_model(X_train,X_validation,y_train,y_validation,hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conculsion\n",
    "## suppost to be look like\n",
    "I want to run and compare hybrid in difference layer, but got a lot of bugs in the middle.\n",
    "but I got a double bug when I want to impliment quantum layer to the circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.52868549391002\n",
      "ground_state [-8.10967563]\n"
     ]
    }
   ],
   "source": [
    "def predict_ground_state(matrics):\n",
    "    #x = feature_select.transform(matrics)\n",
    "    x_filtered = feature_select.transform(matrics.reshape(1,-1))\n",
    "    x_tensor = torch.tensor(x_filtered,device=device).float()\n",
    "    prediction = model(x_tensor)\n",
    "    px = scaler.inverse_transform(prediction.cpu().detach().numpy())\n",
    "    return px[0][0]\n",
    "\n",
    "x = random.sample(range(1,X_test.shape[0]),1)\n",
    "#print(X[0,:])\n",
    "print(predict_ground_state(X[int(x[0]),:]))\n",
    "print('ground_state',y[int(x[0]),:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
